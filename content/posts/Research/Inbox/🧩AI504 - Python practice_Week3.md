## ğŸ¯Goal
- basic concepts of PyTorch Framework
- Â simple logistic regression and multinomial logistic regression (softmax) with PyTorch
- simple linear model and multi-layer perceptron (MLP)
---
# Contents
## Numpy vs PyTorch tensor
### 1. Same operations w/ identical grammar
#### shape vs size()
numpyëŠ” `np_array_1.shape` ì²˜ëŸ¼ shape í•¨ìˆ˜ë¥¼ ì¨ì•¼í•˜ëŠ”ë°, PyTorchëŠ” `torch_tensor_1.shape` `torch_tensor_1.size()` ë‘˜ ë‹¤ ê°™ì€ ê²°ê³¼ë¥¼ ë‚´ë³´ë‚¸ë‹¤.

### 2. Same operations with different grammar
#### np.concatenate vs torch.cat
[[ğŸ§©AI504 - Python practice_Week1#Stack, concatenation]]

- axis(numpy) and dim(torch)ëŠ” identicalí•˜ë‹¤.
```PyTorch
torch_concate=torch.cat([torch_tensor_1, torch_tensor_2], dim=0)
```

#### reshape (np) vs view (PyTorch)

```PyTorch
torch_reshaped = torch_concate.view(4,2)
```

### 3. Different operations with same grammar
#### repeat
repeatì„ ê·¸ëƒ¥ expansionì´ë¼ ìƒê°í•˜ì. 
python : ì›ì†Œë³„ë¡œ expansion
PyTorch : í•˜ë‚˜ì˜ ë°°ì—´ì„ ê¸°ë³¸ ë‹¨ìœ„ë¡œ expansion
```Python
x = np.array([1,2,3])
x_repeat = x.repeat(2) #[1 1 2 2 3 3]

x = torch.tensor([1,2,3])
x_repeat = x.repeat(2) # [1 2 3 1 2 3]

######
x_repeat = x.view(3,1) # [1],[2],[3]
x_repeat = x_repeat.repeat(2,3) ## expansionì´ë¼ ìƒê°í•˜ë©´ ë¨.
x_repeat = x_repeat.view(-1) # [1,1,1,2,2,2,3,3,3,1,1,1,2,2,2,3,3,3]
```

#### stack 
torch.cat() : ì£¼ì–´ì§„ ì°¨ì›ì„ ê¸°ì¤€ìœ¼ë¡œ í…ì„œë“¤ì„ ë¶™ì´ê¸°
torch.stack() : ìƒˆë¡œìš´ ì°¨ì›ìœ¼ë¡œ í…ì„œë¥¼ ë¶™ì´ê¸°.

```PyTorch
x = torch.tensor([1,2,3])
x_repeat = x.repeat(4) # [1,2,3,1,2,3,1,2,3,1,2,3]
x_stack = torch.stack([x, x, x, x]) #[[1,2,3],[1,2,3],[1,2,3],[1,2,3]]

```


## Tensor operations under GPU utilization

```
a = torch.ones(3)
b = torch.randn(100,50,3) # 100(row)*50(column)*3(layer)ë¡œ ë§Œë“¦ # rgb ì´ë¯¸ì§€ë¡œ ìƒê°ì„ í•˜ë©´ ë˜ë‚˜ë´, 100ê°œì˜ sample, 50ê°œì˜ íŠ¹ì„±(ìƒìƒ, ë°ê¸°), 3ê°œì˜ layer(rgb)
b = torch.randn(4,2,3) # 4ê°œì˜ layer, 2(row)*3(column)
```