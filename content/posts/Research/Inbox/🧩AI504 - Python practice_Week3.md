## ðŸŽ¯Goal
- basic concepts of PyTorch Framework
- Â simple logistic regression and multinomial logistic regression (softmax) with PyTorch
- simple linear model and multi-layer perceptron (MLP)
- reference : [Jinsol Kim](https://gaussian37.github.io/dl-pytorch-snippets/)
	- GPUì— ìžˆì„ ë•Œì—ëŠ” CPUë¡œ ë³€ê²½ í›„ npë¡œ ë³€ê²½í•´ì•¼í•œë‹¤.
---
# Contents
## Numpy vs PyTorch tensor
### 1. Same operations w/ identical grammar
#### shape vs size()
numpyëŠ” `np_array_1.shape` ì²˜ëŸ¼ shape í•¨ìˆ˜ë¥¼ ì¨ì•¼í•˜ëŠ”ë°, PyTorchëŠ” `torch_tensor_1.shape` `torch_tensor_1.size()` ë‘˜ ë‹¤ ê°™ì€ ê²°ê³¼ë¥¼ ë‚´ë³´ë‚¸ë‹¤.

### 2. Same operations with different grammar
#### np.concatenate vs torch.cat
[[ðŸ§©AI504 - Python practice_Week1#Stack, concatenation]]

- axis(numpy) and dim(torch)ëŠ” identicalí•˜ë‹¤.
```PyTorch
torch_concate=torch.cat([torch_tensor_1, torch_tensor_2], dim=0)
```

#### reshape (np) vs view (PyTorch)

```PyTorch
torch_reshaped = torch_concate.view(4,2)
```

### 3. Different operations with same grammar
#### repeat
repeatì„ ê·¸ëƒ¥ expansionì´ë¼ ìƒê°í•˜ìž. 
python : ì›ì†Œë³„ë¡œ expansion
PyTorch : í•˜ë‚˜ì˜ ë°°ì—´ì„ ê¸°ë³¸ ë‹¨ìœ„ë¡œ expansion
```Python
x = np.array([1,2,3])
x_repeat = x.repeat(2) #[1 1 2 2 3 3]

x = torch.tensor([1,2,3])
x_repeat = x.repeat(2) # [1 2 3 1 2 3]

######
x_repeat = x.view(3,1) # [1],[2],[3]
x_repeat = x_repeat.repeat(2,3) ## expansionì´ë¼ ìƒê°í•˜ë©´ ë¨.
x_repeat = x_repeat.view(-1) # [1,1,1,2,2,2,3,3,3,1,1,1,2,2,2,3,3,3]
```

#### stack 
torch.cat() : ì£¼ì–´ì§„ ì°¨ì›ì„ ê¸°ì¤€ìœ¼ë¡œ í…ì„œë“¤ì„ ë¶™ì´ê¸°
torch.stack() : ìƒˆë¡œìš´ ì°¨ì›ìœ¼ë¡œ í…ì„œë¥¼ ë¶™ì´ê¸°.

```PyTorch
x = torch.tensor([1,2,3])
x_repeat = x.repeat(4) # [1,2,3,1,2,3,1,2,3,1,2,3]
x_stack = torch.stack([x, x, x, x]) #[[1,2,3],[1,2,3],[1,2,3],[1,2,3]]

```


## Tensor operations under GPU utilization

```
a = torch.ones(3)
b = torch.randn(100,50,3) # 100(row)*50(column)*3(layer)ë¡œ ë§Œë“¦ # rgb ì´ë¯¸ì§€ë¡œ ìƒê°ì„ í•˜ë©´ ë˜ë‚˜ë´, 100ê°œì˜ sample, 50ê°œì˜ íŠ¹ì„±(ìƒìƒ, ë°ê¸°), 3ê°œì˜ layer(rgb)
b = torch.randn(4,2,3) # normal distributionìœ¼ë¡œ 4ê°œì˜ layer, 2(row)*3(column)
```

![[Pasted image 20241010110802.png]]

## Autograd
`x = torch.ones(2,2, requires_grad=True)` : requires_grad ì¼œì¤˜ì•¼ í•´ë‹¹ ë³€ìˆ˜ì— ëŒ€í•œ ì—°ì‚°ì„ trackingí•¨.
ë§ˆì§€ë§‰ loss functionì´ë“  í–‰ë ¬ì—ì„œ .backward()ë¥¼ í•´ì£¼ë©´ gradientë¥¼ ìžë™ì ìœ¼ë¡œ ê³„ì‚°í•¨.
`out.backward()` ì´í›„ , `print(z.grad)`

ê·¼ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ì—°ì‚° history trackingì„ ê¸ˆì§€í•˜ê³  ì‹¶ìœ¼ë©´, `torch.no_grad()`ë¥¼ ì‚¬ìš©í•˜ë©´ ë¨. â†’ Testset ëŒë¦´ ë•Œ!

```PyTorch
with torch.no_grad(): 
	x = torch.ones(2,2, requires_grad=True)
	y = x + 2
	z = y * y * 3
	out = z.mean()
```


## nn.Module

```PyTorch
class Model(nn.Module):

def __init__(self, input_dim, output_dim, hidden_dim):
	super(Model, self).__init__()
	self.linear_1 = nn.Linear(input_dim, hidden_dim)
	self.linear_2 = nn.Linear(hidden_dim, output_dim)
	self.relu = nn.ReLU()

def forward(self, x):
	x = self.linear_1(x)
	x = self.relu(x) # Activation function
	x = self.linear_2(x)

return x
```

##### Train
```PyTorch
model = Multinomial_logistic_regression(784, 10) # init(784, 10)
model = model.to('cuda')
optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9)

# Loss function define (we use cross-entropy)

loss_fn = nn.CrossEntropyLoss()

  
#Train the model
total_step = len(train_loader)

for epoch in range(10):
	for i, (images, labels) in enumerate(train_loader): # mini batch for loop
		# upload to gpu
		images = images.reshape(-1, 28*28).to('cuda') # -1ì´ batch_size
		labels = labels.to('cuda')
		
		# Forward
		outputs = model(images) # forwardI(images): get prediction, model(images)í•˜ë©´ ìžë™ì ìœ¼ë¡œ forwardí•¨ìˆ˜ê°€ í˜¸ì¶œì´ ëœëŒ€.
		
		loss = loss_fn(outputs, labels) # calculate the loss (cross entropy loss) with ground truth & prediction value
		
		# Backward and optimize
		optimizer.zero_grad()
		loss.backward() # automatic gradient calculation (autograd)
		optimizer.step() # update model parameter with requires_grad=True
		
		if (i+1) % 100 == 0:
		print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
		.format(epoch+1, 10, i+1, total_step, loss.item()))
```

##### Test
```PyTorch
# Test the model
# In test phase, we don't need to compute gradients (for memory efficiency)

with torch.no_grad():
	correct = 0
	total = 0
	for images, labels in test_loader:
	
		images = images.reshape(-1, 28*28).to('cuda')
		labels = labels.to('cuda')
		outputs = model(images)
		_, predicted = torch.max(outputs.data, 1) # classification -> get the label prediction of top 1
		total += labels.size(0)
		correct += (predicted == labels).sum().item()
	
	print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))
```